{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named deflection",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8a35d20a543c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeflection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minitialize_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named deflection"
     ]
    }
   ],
   "source": [
    "__author__ = 'kjoseph'\n",
    "from array import array\n",
    "\n",
    "from deflection import *\n",
    "from initialize_functions import *\n",
    "from functions import *\n",
    "import scipy.spatial\n",
    "FUNDAMENTALS = ['ae','ap','aa','be','bp','ba','oe','op','oa']\n",
    "\n",
    "EQUATION_FILE = \"../data/sexes_avg_new.txt\"\n",
    "\n",
    "\n",
    "t,M = get_t_and_M_from_file(EQUATION_FILE, FUNDAMENTALS)\n",
    "deflection_computation = get_coefficients(M,t)\n",
    "H = np.dot(np.vstack((np.identity(9),-M)), np.hstack((np.identity(9),-M.transpose())))\n",
    "S_a,g_a = compute_s_and_g(M,t,'a',EPA_LIST)\n",
    "S_b,g_b = compute_s_and_g(M,t,'b',EPA_LIST)\n",
    "S_o,g_o = compute_s_and_g(M,t,'o',EPA_LIST)\n",
    "##TEST DEFLECTION\n",
    "\n",
    "pm1 = ParameterStruct('t', 1,3,3,[0.556179039209, 1],[0.748935820403, 1],[0.608249328496,1],1,1)\n",
    "pm2 = ParameterStruct('t', 1,3,3,[-0.42344161868, 1],[-0.659712135792, 1],[0.192399427834,1],1,1)\n",
    "opt_v = get_optimal(M,t,H,S_b,g_b,['a','o'], pm1,pm2)\n",
    "\n",
    "for v in [[-0.447532713413, -0.356447458267, 0.429316014051], [-0.069087468087,0.258912533522,0.0270347073674]]:\n",
    "    print scipy.spatial.distance.cosine(opt_v,v)\n",
    "\n",
    "deflection_computation = np.array(deflection_computation, dtype=np.object_)\n",
    "\n",
    "\n",
    "vals_list = array('f',[2.45, 1.75, 0.29,1.85, 1.65, 0.30,1.49,.31,.75])\n",
    "for x in compute_deflection_bad(vals_list,deflection_computation):\n",
    "    print 'x: ', x\n",
    "\n",
    "# np.random.shuffle(M)\n",
    "# deflection_computation = get_coefficients(M, t)\n",
    "# print 'shuffled'\n",
    "# for x in compute_deflection_bad(vals_list,deflection_computation):\n",
    "#     print x\n",
    "#for fundamental_index in range(N_FUNDAMENTALS):\n",
    "#    c0, c1, c2 = get_constants_for_fundamental(fundamental_index,deflection_computation,value_list)\n",
    "#    print FUNDAMENTALS[fundamental_index], -c1/(2*c0), c1, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.155+0.41*ae+0.42*be-0.02*bp-0.1*ba+0.03*oe+0.06*op+0.05*be*ae+0.03*ae*op+0.12*be*oe-0.05*be*op-0.05*oe*bp+0.03*be*oe*ae-0.02*be*ae*op', '-0.1+0.56*ap+0.06*aa-0.105*be+0.44*bp+0.04*oe-0.05*ap*bp+0.01*be*oe', '+0.14+0.05*ae+0.705*aa-0.06*be+0.29*ba-0.06*aa*ba', '-0.14+0.11*ae+0.555*be-0.12*ba+0.05*op+0.11*be*oe-0.05*be*op-0.02*oe*bp+0.02*be*oe*ae', '+0.06+0.16*ap-0.15*be+0.685*bp+0.03*oe-0.015*op+0.01*be*ae+0.02*be*oe', '+0.17+0.02*ae-0.06*ap+0.3*aa+0.04*be+0.64*ba', '+0.025+0.11*be+0.61*oe-0.01*oa+0.03*be*ae+0.04*be*oe-0.03*oe*bp', '-0.395+0.15*be-0.11*bp-0.115*oe+0.66*op+0.07*oa+0.03*be*oe+0.03*be*op-0.05*bp*op', '-0.035+0.02*be+0.03*oe-0.05*op+0.745*oa']\n"
     ]
    }
   ],
   "source": [
    "def get_t_and_M_from_file(eq_filename, fundamentals,spl_char= \"\\t\"):\n",
    "    M = []\n",
    "    t = []\n",
    "    equation_file = open(eq_filename)\n",
    "    i = 0\n",
    "    for line in equation_file:\n",
    "        t.append(set())\n",
    "        line_spl = [l.strip() for l in line.split(spl_char)]\n",
    "        M.append([float(x) for x in line_spl[1:]])\n",
    "\n",
    "        coeff = line_spl[0].replace(\"Z\",\"\")\n",
    "        for j in range(len(coeff)):\n",
    "            if coeff[j] == '1':\n",
    "                t[i].add(fundamentals[j])\n",
    "        i+=1\n",
    "\n",
    "    equation_file.close()\n",
    "    return t, np.array(M)\n",
    "\n",
    "FUNDAMENTALS = ['ae','ap','aa','be','bp','ba','oe','op','oa']\n",
    "\n",
    "t, M = get_t_and_M_from_file(\"../data/sexes_avg_new.txt\",FUNDAMENTALS,\"\\t\")\n",
    "\n",
    "fund_eq = [[] for i in range(len(FUNDAMENTALS))]\n",
    "for j in range(len(FUNDAMENTALS)):\n",
    "    for i,coef in enumerate(t):\n",
    "        coef = \"*\".join(coef)\n",
    "        l = M[i,j]\n",
    "        app_str = \"\"\n",
    "        if l > 0:\n",
    "            app_str = \"+\"\n",
    "        if l == 0:\n",
    "            continue\n",
    "        elif coef != '':\n",
    "            fund_eq[j].append(app_str +str(l)+\"*\"+coef)\n",
    "        else:\n",
    "            fund_eq[j].append(app_str+str(l))\n",
    "\n",
    "\n",
    "FUND_EQ_STRS = [\"\".join(x) for x in fund_eq]\n",
    "print FUND_EQ_STRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_constraint_string() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-bff8bc61783b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msymbols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSymbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0meq_constr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_constraint_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconstraint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mequation_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"+\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meq_constr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msympify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_constraint_string() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "from sympy import sympify\n",
    "from sympy.polys import Poly\n",
    "from math import sqrt \n",
    "\n",
    "\n",
    "identities = [1, 2]\n",
    "sentiment_words = [1,4]\n",
    "constraints = [SocialEventConstraint(actor=1, behavior=1, object=2),\n",
    "               EqualityConstraint(identity=1, sentiment_word=1)]\n",
    "               \n",
    "equation_str = ''\n",
    "\n",
    "for starter, list_of_terms in  [['i',identities], ['z',sentiment_words]]:\n",
    "    for term in list_of_terms:\n",
    "        for epa in ['e','p','a']:\n",
    "            id = starter+str(term)+epa\n",
    "            symbols[id] = Symbol(id)\n",
    "\n",
    "eq_constr = [constraint.get_constraint_string() for constraint in constraints]\n",
    "equation_str = \"+\".join(eq_constr) \n",
    "expr = sympify(equation_str)\n",
    "\n",
    "dat = {\"i1e\": 1.39,\n",
    "       \"i1p\": .88,\n",
    "       \"i1a\": 0.96,\n",
    "       \"z1e\": -1.92,\n",
    "       \"z1p\": 1.00,\n",
    "       \"z1a\": 1.62,\n",
    "       \"i2e\": 1.49,\n",
    "       \"i2p\": .31,\n",
    "       \"i2a\": 0.75,\n",
    "      }\n",
    "substitutions = dat.items()\n",
    "expr = expr.subs(substitutions).expand()\n",
    "p = Poly(expr).coeffs()\n",
    "print expr\n",
    "print -p[1]/(2*p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    \n",
    "    def __init__(self, n_identities, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.sentences_for_identity = [list() * n_identities]\n",
    "        for sent_it, sentence in enumerate(sentences):\n",
    "            self.add_sentence(sentence,sent_it)\n",
    "            \n",
    "    def add_sentence(sentence,sent_it=None):\n",
    "        self.sentences.append(sentence)\n",
    "        if not sent_it:\n",
    "            sent_it = len(sentence)\n",
    "        for identity in sentence.identities_contained():\n",
    "            self.sentences_for_identity[identity].append(sent_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_grouped_file\n",
    "\n",
    "for g in d:\n",
    "    construct_sentence\n",
    "    add_to_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-d180dd2c8012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0m_wnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "_wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = {}\n",
    "for x in codecs.open(\"../../../../thesis/thesis_work/lcss_study/data/all_epa_terms.txt\",\"r\",\"utf8\"):\n",
    "    x_spl = x.split(\"\\t\")\n",
    "    sent_dict[x_spl[0]] = [float(z) for z in x_spl[1:]]\n",
    "\n",
    "\n",
    "IDENTITY_LIST_FN = \"../../../../thesis/thesis_work/lcss_study/data/identities_for_study.txt\"\n",
    "identity_set = {x.strip().lower() for x in open(IDENTITY_LIST_FN)}\n",
    "full_set_of_interesting_terms = identity_set|set(sent_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'stick'"
      ]
     },
     "execution_count": 171,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "_wnl.lemmatize(\"stuck\",wn.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'deal with'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-3f7cdd1d7e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deal with'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'deal with'"
     ]
    }
   ],
   "source": [
    "sent_dict['deal with']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_set = {i : x.strip().lower() for i,x in enumerate(open(IDENTITY_LIST_FN))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load_word2vec_format(\"../../../../thesis/thesis_work/identity_extraction/python/gensim_model/glove_twitter_50_raw_model.txt.gz\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "'miracle' in sent_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}